{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import logging  \n",
    ">>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from gensim import corpora, models, similarities  \n",
    ">>>  \n",
    ">>> corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],  \n",
    ">>>           [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)],  \n",
    ">>>           [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],  \n",
    ">>>           [(0, 1.0), (4, 2.0), (7, 1.0)],  \n",
    ">>>           [(3, 1.0), (5, 1.0), (6, 1.0)],  \n",
    ">>>           [(9, 1.0)],  \n",
    ">>>           [(9, 1.0), (10, 1.0)],  \n",
    ">>>           [(9, 1.0), (10, 1.0), (11, 1.0)],  \n",
    ">>>           [(8, 1.0), (10, 1.0), (11, 1.0)]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-29 09:20:05,667 : INFO : collecting document frequencies\n",
      "2017-09-29 09:20:05,669 : INFO : PROGRESS: processing document #0\n",
      "2017-09-29 09:20:05,671 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    ">>> tfidf = models.TfidfModel(corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8075244024440723), (4, 0.5898341626740045)]\n"
     ]
    }
   ],
   "source": [
    ">>> vec = [(0, 1), (4, 1)]  \n",
    ">>> print(tfidf[vec])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    ">>>              \"A survey of user opinion of computer system response time\",\n",
    ">>>              \"The EPS user interface management system\",\n",
    ">>>              \"System and human system engineering testing of EPS\",\n",
    ">>>              \"Relation of user perceived response time to error measurement\",\n",
    ">>>              \"The generation of random binary unordered trees\",\n",
    ">>>              \"The intersection graph of paths in trees\",\n",
    ">>>              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    ">>>              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # 去除停用词并分词\n",
    ">>> # 译者注：这里只是例子，实际上还有其他停用词\n",
    ">>> #         处理中文时，请借助 Py结巴分词 https://github.com/fxsjy/jieba\n",
    ">>> stoplist = set('for a of the and to in'.split())\n",
    ">>> texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    ">>>          for document in documents]\n",
    ">>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts   #[][] text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    ">>> # [][]去除仅出现一次的单词\n",
    ">>> from collections import defaultdict\n",
    ">>> frequency = defaultdict(int)\n",
    ">>> for text in texts:\n",
    ">>>     for token in text:\n",
    ">>>         frequency[token] += 1\n",
    ">>>\n",
    ">>> texts = [[token for token in text if frequency[token] > 1]\n",
    ">>>          for text in texts]\n",
    ">>>\n",
    ">>> from pprint import pprint   # pretty-printer\n",
    ">>> pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-29 09:40:58,348 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-09-29 09:40:58,352 : INFO : built Dictionary(12 unique tokens: ['trees', 'eps', 'survey', 'interface', 'system']...) from 9 documents (total 29 corpus positions)\n",
      "2017-09-29 09:40:58,355 : INFO : saving Dictionary object under /tmp/deerwester.dict, separately None\n",
      "2017-09-29 09:40:58,358 : INFO : saved /tmp/deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['trees', 'eps', 'survey', 'interface', 'system']...)\n"
     ]
    }
   ],
   "source": [
    ">>> dictionary = corpora.Dictionary(texts) #[][]\n",
    ">>> dictionary.save('/tmp/deerwester.dict') # 把字典保存起来，方便以后使用\n",
    ">>> print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = dictionary.token2id #[][]\n",
    "fff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trees': 9, 'eps': 8, 'survey': 7, 'interface': 2, 'system': 5, 'time': 3, 'graph': 10, 'minors': 11, 'user': 6, 'human': 0, 'response': 4, 'computer': 1}\n"
     ]
    }
   ],
   "source": [
    ">>> print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    ">>> new_doc = \"Human computer interaction\"\n",
    ">>> new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    ">>> print(new_vec) # \"interaction\"没有在dictionary中出现，因此忽略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-28 17:34:50,690 : INFO : storing corpus in Matrix Market format to /tmp/deerwester.mm\n",
      "2017-09-28 17:34:50,695 : INFO : saving sparse matrix to /tmp/deerwester.mm\n",
      "2017-09-28 17:34:50,697 : INFO : PROGRESS: saving document #0\n",
      "2017-09-28 17:34:50,700 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2017-09-28 17:34:50,703 : INFO : saving MmCorpus index to /tmp/deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (6, 1), (8, 1)], [(0, 1), (5, 2), (8, 1)], [(3, 1), (4, 1), (6, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(7, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    ">>> corpus = [dictionary.doc2bow(text) for text in texts]\n",
    ">>> corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus) # [][] corpus format存入硬盘，以备后需\n",
    ">>> print(corpus)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-29 14:22:59,494 : INFO : storing corpus in Matrix Market format to /tmp/corpus.mm\n",
      "2017-09-29 14:22:59,500 : INFO : saving sparse matrix to /tmp/corpus.mm\n",
      "2017-09-29 14:22:59,502 : INFO : PROGRESS: saving document #0\n",
      "2017-09-29 14:22:59,504 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2017-09-29 14:22:59,506 : INFO : saving MmCorpus index to /tmp/corpus.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1.0), (1, 1.0), (2, 1.0)], [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)], [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)], [(0, 1.0), (4, 2.0), (7, 1.0)], [(3, 1.0), (5, 1.0), (6, 1.0)], [(9, 1.0)], [(9, 1.0), (10, 1.0)], [(9, 1.0), (10, 1.0), (11, 1.0)], [(8, 1.0), (10, 1.0), (11, 1.0)]]\n"
     ]
    }
   ],
   "source": [
    "corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-29 14:23:49,454 : INFO : collecting document frequencies\n",
      "2017-09-29 14:23:49,457 : INFO : PROGRESS: processing document #0\n",
      "2017-09-29 14:23:49,460 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4472135954999579), (1, 0.8944271909999159)]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[[(0, 1), (1, 2)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-29 12:31:45,041 : INFO : collecting all words and their counts\n",
      "2017-09-29 12:31:45,045 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-29 12:31:45,047 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2017-09-29 12:31:45,048 : INFO : Loading a fresh vocabulary\n",
      "2017-09-29 12:31:45,050 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2017-09-29 12:31:45,051 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2017-09-29 12:31:45,053 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2017-09-29 12:31:45,054 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-09-29 12:31:45,056 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2017-09-29 12:31:45,057 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2017-09-29 12:31:45,058 : INFO : resetting layer weights\n",
      "2017-09-29 12:31:45,060 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-09-29 12:31:45,094 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-29 12:31:45,095 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-29 12:31:45,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-29 12:31:45,097 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-09-29 12:31:45,098 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']] #sentence format\n",
    "\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = model.wv\n",
    "dir(tmp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.14341346e-03,  -3.53613822e-03,   4.26025782e-03,\n",
       "         3.90929263e-03,  -1.14847720e-03,   4.29830328e-03,\n",
       "        -3.93148465e-03,  -4.51363204e-03,   4.21966938e-03,\n",
       "        -2.49997433e-03,   2.43948353e-03,   1.86539686e-03,\n",
       "         2.50470941e-03,  -8.33303493e-04,   4.72105900e-03,\n",
       "        -2.72903848e-03,   2.33246759e-03,   3.95893957e-03,\n",
       "        -2.73914612e-03,  -4.55483003e-03,   2.58737151e-03,\n",
       "        -2.55773170e-03,   6.97944371e-04,   1.44014554e-03,\n",
       "        -2.15786789e-03,  -4.76948591e-03,  -2.21983274e-03,\n",
       "         2.87174573e-03,   1.94301200e-03,  -1.80530432e-03,\n",
       "         3.88266332e-03,  -2.02300027e-03,   3.28199170e-03,\n",
       "        -3.95493844e-04,   9.71500878e-04,  -8.48169031e-04,\n",
       "        -3.83598288e-03,  -1.72207050e-03,  -3.39986640e-03,\n",
       "         2.34451680e-03,  -3.29425698e-03,   4.66897525e-03,\n",
       "        -6.64377701e-04,   1.98037084e-03,  -3.16024991e-03,\n",
       "        -7.24018973e-05,   1.91803440e-03,  -2.49440735e-03,\n",
       "         2.64591858e-04,   4.04083589e-03,  -4.42259060e-03,\n",
       "         1.14865496e-03,  -4.80782334e-03,  -4.46118228e-03,\n",
       "         2.24006129e-03,  -1.98287284e-03,  -1.85600517e-03,\n",
       "        -3.94944893e-03,   2.93213059e-03,  -1.12673512e-03,\n",
       "        -4.39362042e-03,  -3.24071455e-03,  -1.58247363e-03,\n",
       "         3.76595836e-03,   3.98094882e-04,  -3.88635485e-03,\n",
       "         2.02945457e-03,  -3.34135955e-03,   2.63434858e-03,\n",
       "         2.23992602e-03,  -8.31906218e-04,   3.70573509e-03,\n",
       "         2.03556032e-03,  -2.63263029e-03,  -2.06555566e-03,\n",
       "        -1.82424686e-04,  -4.78435587e-03,  -1.64956786e-03,\n",
       "         3.93015519e-03,   3.35382135e-03,   8.42699490e-04,\n",
       "         3.41400760e-03,   2.32997071e-03,   3.49159748e-03,\n",
       "        -1.00857776e-03,  -4.49141348e-03,   2.61906674e-03,\n",
       "         1.13928683e-04,  -2.01765346e-04,  -4.76983283e-03,\n",
       "        -4.80618421e-03,  -1.45679014e-03,  -7.43241748e-04,\n",
       "         3.30940937e-03,  -9.66125168e-04,  -4.77935048e-03,\n",
       "        -2.09720898e-03,   2.62222067e-03,   3.70710180e-03,\n",
       "         2.10508500e-04], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-29 12:32:46,988 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '/some/directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-056264fb93e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMySentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/some/directory'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# a memory-friendly iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss)\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             self.train(sentences, total_examples=self.corpus_count, epochs=self.iter,\n\u001b[0;32m    505\u001b[0m                        start_alpha=self.alpha, end_alpha=self.min_alpha)\n",
      "\u001b[1;32md:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \"\"\"\n\u001b[1;32m--> 577\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initial survey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build tables & arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-056264fb93e4>\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/some/directory'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    " \n",
    "sentences = MySentences('/some/directory') # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
